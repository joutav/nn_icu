{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "176057b490ab50ebe67202604d51d7e75c342e0a6cf799cbf4ed6c283c494768"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modulos.dataset import Uti\n",
    "from src.modulos.rede import ModeloNeural\n",
    "\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    batch_size=10,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    weight_decay = 1e-3,\n",
    "    lr = 1e-4,\n",
    "    epoch = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Uti('../dados/x_train.csv', '../dados/y_train.csv')\n",
    "test_set = Uti('../dados/x_test.csv', '../dados/y_test.csv')\n",
    "\n",
    "train_loader = DataLoader(train_set, args['batch_size'])\n",
    "test_loader = DataLoader(test_set, args['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ModeloNeural(\n",
       "  (features): Sequential(\n",
       "    (0): Linear(in_features=228, out_features=456, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=456, out_features=912, bias=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=912, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "input_size = 228\n",
    "hidden_size = 456\n",
    "second_hidden = 912\n",
    "out_size = 2\n",
    "\n",
    "net = ModeloNeural(input_size, hidden_size, second_hidden, out_size)\n",
    "net = net.to(args['device'])\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "otimizador = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "perda = nn.CrossEntropyLoss().to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, epoch : int):\n",
    "    start = time.time() # Marcador de tempo\n",
    "    net.train()\n",
    "\n",
    "    perda_epoca = []\n",
    "    pred_list, rotulo_list = [], []\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # Input\n",
    "        dado, rotulo = batch\n",
    "        \n",
    "        #Cast para GPU\n",
    "        dado = dado.to(args['device'])\n",
    "        rotulo = rotulo.to(args['device']).squeeze()\n",
    "\n",
    "        # zerar parametros gradientes\n",
    "        otimizador.zero_grad()\n",
    "\n",
    "        # forward + \n",
    "        saida = net(dado)\n",
    "        loss = perda(saida, rotulo)\n",
    "        \n",
    "\n",
    "        # Salvando dados\n",
    "        perda_epoca.append(loss.cpu().data)         # Loss\n",
    "        _, predict = torch.max(saida, 1)            # Predicao\n",
    "        pred_list.append(predict.cpu().numpy())     \n",
    "        rotulo_list.append(rotulo.cpu().numpy())    # rotulos\n",
    "\n",
    "        \n",
    "        # backward + optmize\n",
    "        loss.backward()\n",
    "        otimizador.step()\n",
    "    \n",
    "\n",
    "      \n",
    "    pred_list = np.concatenate(pred_list)\n",
    "    rotulo_list = np.concatenate(rotulo_list)\n",
    "\n",
    "    acuracia = accuracy_score(rotulo_list, pred_list) * 100\n",
    "    perda_epoca = np.array(perda_epoca)\n",
    "\n",
    "    end = time.time()  \n",
    "\n",
    "    print(f'Epoch {epoch:>4} - train_score {acuracia:>6.2f} - loss {perda_epoca.mean():0.2f} - time {end-start:0.2f}', end=' // ')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valida(net, test_loader):\n",
    "    start = time.time()\n",
    "    net.eval()\n",
    "    \n",
    "    perda_epoca = []\n",
    "    pred_list, rotulo_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            dado, rotulo = batch\n",
    "            \n",
    "            # Cast para GPU\n",
    "            dado = dado.to(args['device'])\n",
    "            rotulo = rotulo.to(args['device']).squeeze()\n",
    "\n",
    "            # Forward\n",
    "            predicao = net(dado)\n",
    "            loss = perda(predicao, rotulo)\n",
    "\n",
    "            # Salvando dados\n",
    "            perda_epoca.append(loss.cpu().data)\n",
    "            _, predict = torch.max(predicao, 1)             \n",
    "            pred_list.append(predict.cpu().numpy())     \n",
    "            rotulo_list.append(rotulo.cpu().numpy())\n",
    "\n",
    "     \n",
    "    pred_list = np.concatenate(pred_list)\n",
    "    rotulo_list = np.concatenate(rotulo_list)\n",
    "\n",
    "    acuracia = accuracy_score(rotulo_list, pred_list) * 100\n",
    "    perda_epoca = np.array(perda_epoca)\n",
    "\n",
    "    end = time.time()   \n",
    "\n",
    "    print(f'val_score {acuracia:>6.2f} - val_loss {perda_epoca.mean():0.2f} - time {end-start:0.2f}') \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".65 - val_loss 0.31 - time 0.04\n",
      "Epoch  308 - train_score  96.74 - loss 0.09 - time 0.26 // val_score  91.23 - val_loss 0.32 - time 0.04\n",
      "Epoch  309 - train_score  96.85 - loss 0.09 - time 0.30 // val_score  91.94 - val_loss 0.33 - time 0.04\n",
      "Epoch  310 - train_score  96.64 - loss 0.09 - time 0.33 // val_score  90.28 - val_loss 0.34 - time 0.03\n",
      "Epoch  311 - train_score  97.46 - loss 0.08 - time 0.25 // val_score  91.00 - val_loss 0.34 - time 0.04\n",
      "Epoch  312 - train_score  96.24 - loss 0.10 - time 0.29 // val_score  92.18 - val_loss 0.32 - time 0.04\n",
      "Epoch  313 - train_score  96.24 - loss 0.09 - time 0.33 // val_score  92.89 - val_loss 0.32 - time 0.04\n",
      "Epoch  314 - train_score  95.73 - loss 0.11 - time 0.28 // val_score  92.18 - val_loss 0.32 - time 0.04\n",
      "Epoch  315 - train_score  96.54 - loss 0.08 - time 0.29 // val_score  93.60 - val_loss 0.31 - time 0.04\n",
      "Epoch  316 - train_score  97.05 - loss 0.08 - time 0.34 // val_score  90.28 - val_loss 0.35 - time 0.03\n",
      "Epoch  317 - train_score  96.85 - loss 0.08 - time 0.24 // val_score  92.65 - val_loss 0.32 - time 0.04\n",
      "Epoch  318 - train_score  97.05 - loss 0.08 - time 0.33 // val_score  91.47 - val_loss 0.34 - time 0.05\n",
      "Epoch  319 - train_score  95.93 - loss 0.11 - time 0.39 // val_score  91.00 - val_loss 0.33 - time 0.04\n",
      "Epoch  320 - train_score  97.46 - loss 0.08 - time 0.29 // val_score  91.71 - val_loss 0.35 - time 0.06\n",
      "Epoch  321 - train_score  97.86 - loss 0.07 - time 0.32 // val_score  91.94 - val_loss 0.34 - time 0.15\n",
      "Epoch  322 - train_score  97.36 - loss 0.07 - time 0.29 // val_score  91.71 - val_loss 0.33 - time 0.04\n",
      "Epoch  323 - train_score  96.34 - loss 0.08 - time 0.36 // val_score  91.94 - val_loss 0.35 - time 0.05\n",
      "Epoch  324 - train_score  97.25 - loss 0.08 - time 0.32 // val_score  90.76 - val_loss 0.38 - time 0.04\n",
      "Epoch  325 - train_score  96.74 - loss 0.09 - time 0.28 // val_score  91.47 - val_loss 0.36 - time 0.04\n",
      "Epoch  326 - train_score  96.54 - loss 0.09 - time 0.33 // val_score  90.52 - val_loss 0.34 - time 0.04\n",
      "Epoch  327 - train_score  96.44 - loss 0.09 - time 0.35 // val_score  90.05 - val_loss 0.33 - time 0.04\n",
      "Epoch  328 - train_score  96.95 - loss 0.09 - time 0.27 // val_score  90.76 - val_loss 0.33 - time 0.04\n",
      "Epoch  329 - train_score  96.54 - loss 0.08 - time 0.33 // val_score  92.42 - val_loss 0.32 - time 0.07\n",
      "Epoch  330 - train_score  96.74 - loss 0.09 - time 0.31 // val_score  89.10 - val_loss 0.37 - time 0.05\n",
      "Epoch  331 - train_score  96.34 - loss 0.09 - time 0.27 // val_score  91.71 - val_loss 0.31 - time 0.04\n",
      "Epoch  332 - train_score  97.25 - loss 0.08 - time 0.35 // val_score  92.65 - val_loss 0.32 - time 0.05\n",
      "Epoch  333 - train_score  96.95 - loss 0.09 - time 0.27 // val_score  92.42 - val_loss 0.34 - time 0.04\n",
      "Epoch  334 - train_score  97.97 - loss 0.07 - time 0.30 // val_score  92.18 - val_loss 0.33 - time 0.05\n",
      "Epoch  335 - train_score  97.97 - loss 0.08 - time 0.37 // val_score  91.23 - val_loss 0.35 - time 0.05\n",
      "Epoch  336 - train_score  97.05 - loss 0.08 - time 0.27 // val_score  92.89 - val_loss 0.33 - time 0.03\n",
      "Epoch  337 - train_score  97.76 - loss 0.07 - time 0.29 // val_score  92.18 - val_loss 0.32 - time 0.04\n",
      "Epoch  338 - train_score  97.36 - loss 0.07 - time 0.35 // val_score  92.18 - val_loss 0.33 - time 0.04\n",
      "Epoch  339 - train_score  97.25 - loss 0.08 - time 0.27 // val_score  91.94 - val_loss 0.34 - time 0.04\n",
      "Epoch  340 - train_score  98.07 - loss 0.06 - time 0.27 // val_score  92.65 - val_loss 0.34 - time 0.04\n",
      "Epoch  341 - train_score  96.74 - loss 0.09 - time 0.32 // val_score  92.18 - val_loss 0.35 - time 0.05\n",
      "Epoch  342 - train_score  97.15 - loss 0.08 - time 0.26 // val_score  92.89 - val_loss 0.32 - time 0.05\n",
      "Epoch  343 - train_score  96.54 - loss 0.10 - time 0.28 // val_score  91.71 - val_loss 0.34 - time 0.04\n",
      "Epoch  344 - train_score  97.36 - loss 0.08 - time 0.31 // val_score  91.47 - val_loss 0.33 - time 0.05\n",
      "Epoch  345 - train_score  97.56 - loss 0.08 - time 0.25 // val_score  93.13 - val_loss 0.34 - time 0.04\n",
      "Epoch  346 - train_score  96.85 - loss 0.09 - time 0.28 // val_score  92.42 - val_loss 0.33 - time 0.05\n",
      "Epoch  347 - train_score  97.36 - loss 0.07 - time 0.34 // val_score  92.89 - val_loss 0.34 - time 0.06\n",
      "Epoch  348 - train_score  96.44 - loss 0.09 - time 0.26 // val_score  92.65 - val_loss 0.34 - time 0.04\n",
      "Epoch  349 - train_score  98.17 - loss 0.06 - time 0.29 // val_score  92.42 - val_loss 0.32 - time 0.04\n",
      "Epoch  350 - train_score  98.07 - loss 0.06 - time 0.31 // val_score  93.13 - val_loss 0.35 - time 0.05\n",
      "Epoch  351 - train_score  97.86 - loss 0.06 - time 0.27 // val_score  90.76 - val_loss 0.35 - time 0.04\n",
      "Epoch  352 - train_score  97.15 - loss 0.08 - time 0.28 // val_score  92.18 - val_loss 0.32 - time 0.04\n",
      "Epoch  353 - train_score  97.66 - loss 0.07 - time 0.29 // val_score  92.65 - val_loss 0.33 - time 0.06\n",
      "Epoch  354 - train_score  96.64 - loss 0.08 - time 0.26 // val_score  93.13 - val_loss 0.33 - time 0.04\n",
      "Epoch  355 - train_score  96.95 - loss 0.08 - time 0.27 // val_score  91.71 - val_loss 0.33 - time 0.05\n",
      "Epoch  356 - train_score  96.85 - loss 0.08 - time 0.29 // val_score  91.71 - val_loss 0.33 - time 0.05\n",
      "Epoch  357 - train_score  96.85 - loss 0.09 - time 0.27 // val_score  92.18 - val_loss 0.33 - time 0.04\n",
      "Epoch  358 - train_score  98.17 - loss 0.07 - time 0.30 // val_score  94.08 - val_loss 0.32 - time 0.04\n",
      "Epoch  359 - train_score  97.46 - loss 0.07 - time 0.29 // val_score  92.42 - val_loss 0.34 - time 0.07\n",
      "Epoch  360 - train_score  97.05 - loss 0.07 - time 0.28 // val_score  93.84 - val_loss 0.33 - time 0.04\n",
      "Epoch  361 - train_score  96.44 - loss 0.08 - time 0.28 // val_score  90.76 - val_loss 0.37 - time 0.04\n",
      "Epoch  362 - train_score  95.73 - loss 0.10 - time 0.30 // val_score  91.47 - val_loss 0.33 - time 0.06\n",
      "Epoch  363 - train_score  96.13 - loss 0.10 - time 0.28 // val_score  93.60 - val_loss 0.32 - time 0.04\n",
      "Epoch  364 - train_score  96.85 - loss 0.09 - time 0.28 // val_score  91.47 - val_loss 0.33 - time 0.05\n",
      "Epoch  365 - train_score  96.85 - loss 0.08 - time 0.28 // val_score  93.13 - val_loss 0.33 - time 0.05\n",
      "Epoch  366 - train_score  97.66 - loss 0.07 - time 0.28 // val_score  91.94 - val_loss 0.32 - time 0.04\n",
      "Epoch  367 - train_score  97.25 - loss 0.08 - time 0.27 // val_score  91.94 - val_loss 0.33 - time 0.04\n",
      "Epoch  368 - train_score  97.76 - loss 0.07 - time 0.30 // val_score  92.65 - val_loss 0.33 - time 0.04\n",
      "Epoch  369 - train_score  97.97 - loss 0.07 - time 0.30 // val_score  92.18 - val_loss 0.33 - time 0.03\n",
      "Epoch  370 - train_score  98.37 - loss 0.06 - time 0.26 // val_score  92.42 - val_loss 0.34 - time 0.05\n",
      "Epoch  371 - train_score  96.74 - loss 0.10 - time 0.30 // val_score  93.13 - val_loss 0.32 - time 0.04\n",
      "Epoch  372 - train_score  97.76 - loss 0.08 - time 0.31 // val_score  91.47 - val_loss 0.37 - time 0.04\n",
      "Epoch  373 - train_score  97.25 - loss 0.08 - time 0.28 // val_score  93.13 - val_loss 0.31 - time 0.05\n",
      "Epoch  374 - train_score  96.95 - loss 0.07 - time 0.30 // val_score  92.18 - val_loss 0.35 - time 0.04\n",
      "Epoch  375 - train_score  97.76 - loss 0.08 - time 0.30 // val_score  92.18 - val_loss 0.33 - time 0.04\n",
      "Epoch  376 - train_score  97.56 - loss 0.08 - time 0.26 // val_score  91.71 - val_loss 0.34 - time 0.04\n",
      "Epoch  377 - train_score  97.05 - loss 0.08 - time 0.32 // val_score  92.18 - val_loss 0.34 - time 0.04\n",
      "Epoch  378 - train_score  98.37 - loss 0.06 - time 0.30 // val_score  91.94 - val_loss 0.33 - time 0.04\n",
      "Epoch  379 - train_score  97.86 - loss 0.07 - time 0.25 // val_score  92.42 - val_loss 0.34 - time 0.04\n",
      "Epoch  380 - train_score  97.46 - loss 0.07 - time 0.31 // val_score  92.89 - val_loss 0.33 - time 0.04\n",
      "Epoch  381 - train_score  97.25 - loss 0.08 - time 0.30 // val_score  90.76 - val_loss 0.36 - time 0.04\n",
      "Epoch  382 - train_score  97.66 - loss 0.07 - time 0.24 // val_score  92.89 - val_loss 0.33 - time 0.04\n",
      "Epoch  383 - train_score  97.66 - loss 0.06 - time 0.33 // val_score  92.18 - val_loss 0.34 - time 0.04\n",
      "Epoch  384 - train_score  98.07 - loss 0.06 - time 0.31 // val_score  92.89 - val_loss 0.35 - time 0.04\n",
      "Epoch  385 - train_score  97.56 - loss 0.06 - time 0.25 // val_score  91.94 - val_loss 0.33 - time 0.04\n",
      "Epoch  386 - train_score  96.95 - loss 0.08 - time 0.29 // val_score  91.94 - val_loss 0.34 - time 0.05\n",
      "Epoch  387 - train_score  97.46 - loss 0.07 - time 0.33 // val_score  92.89 - val_loss 0.32 - time 0.04\n",
      "Epoch  388 - train_score  97.76 - loss 0.07 - time 0.25 // val_score  92.42 - val_loss 0.34 - time 0.04\n",
      "Epoch  389 - train_score  96.95 - loss 0.07 - time 0.32 // val_score  92.65 - val_loss 0.33 - time 0.04\n",
      "Epoch  390 - train_score  97.86 - loss 0.07 - time 0.32 // val_score  92.18 - val_loss 0.34 - time 0.03\n",
      "Epoch  391 - train_score  98.37 - loss 0.06 - time 0.25 // val_score  91.00 - val_loss 0.35 - time 0.04\n",
      "Epoch  392 - train_score  97.36 - loss 0.08 - time 0.31 // val_score  90.76 - val_loss 0.34 - time 0.04\n",
      "Epoch  393 - train_score  97.25 - loss 0.07 - time 0.32 // val_score  92.65 - val_loss 0.33 - time 0.04\n",
      "Epoch  394 - train_score  96.44 - loss 0.07 - time 0.25 // val_score  91.23 - val_loss 0.33 - time 0.04\n",
      "Epoch  395 - train_score  98.07 - loss 0.06 - time 0.28 // val_score  91.71 - val_loss 0.34 - time 0.05\n",
      "Epoch  396 - train_score  98.07 - loss 0.07 - time 0.34 // val_score  90.05 - val_loss 0.39 - time 0.04\n",
      "Epoch  397 - train_score  98.07 - loss 0.07 - time 0.25 // val_score  92.65 - val_loss 0.34 - time 0.04\n",
      "Epoch  398 - train_score  97.66 - loss 0.06 - time 0.34 // val_score  91.71 - val_loss 0.36 - time 0.06\n",
      "Epoch  399 - train_score  97.25 - loss 0.07 - time 0.39 // val_score  92.65 - val_loss 0.33 - time 0.04\n",
      "Epoch  400 - train_score  97.97 - loss 0.06 - time 0.27 // val_score  92.18 - val_loss 0.35 - time 0.05\n",
      "Epoch  401 - train_score  98.98 - loss 0.05 - time 0.37 // val_score  91.47 - val_loss 0.36 - time 0.06\n",
      "Epoch  402 - train_score  97.25 - loss 0.07 - time 0.30 // val_score  92.89 - val_loss 0.33 - time 0.05\n",
      "Epoch  403 - train_score  97.66 - loss 0.06 - time 0.30 // val_score  91.94 - val_loss 0.34 - time 0.04\n",
      "Epoch  404 - train_score  97.25 - loss 0.07 - time 0.37 // val_score  92.89 - val_loss 0.33 - time 0.04\n",
      "Epoch  405 - train_score  97.97 - loss 0.07 - time 0.25 // val_score  92.65 - val_loss 0.33 - time 0.04\n",
      "Epoch  406 - train_score  96.54 - loss 0.08 - time 0.28 // val_score  92.89 - val_loss 0.33 - time 0.04\n",
      "Epoch  407 - train_score  97.76 - loss 0.06 - time 0.34 // val_score  93.36 - val_loss 0.32 - time 0.05\n",
      "Epoch  408 - train_score  97.15 - loss 0.07 - time 0.25 // val_score  93.13 - val_loss 0.33 - time 0.04\n",
      "Epoch  409 - train_score  98.58 - loss 0.06 - time 0.27 // val_score  93.84 - val_loss 0.34 - time 0.04\n",
      "Epoch  410 - train_score  97.97 - loss 0.07 - time 0.31 // val_score  92.42 - val_loss 0.33 - time 0.06\n",
      "Epoch  411 - train_score  97.56 - loss 0.06 - time 0.26 // val_score  93.36 - val_loss 0.32 - time 0.04\n",
      "Epoch  412 - train_score  98.27 - loss 0.06 - time 0.27 // val_score  93.36 - val_loss 0.32 - time 0.05\n",
      "Epoch  413 - train_score  97.76 - loss 0.08 - time 0.31 // val_score  91.94 - val_loss 0.32 - time 0.05\n",
      "Epoch  414 - train_score  97.25 - loss 0.07 - time 0.27 // val_score  92.65 - val_loss 0.32 - time 0.03\n",
      "Epoch  415 - train_score  97.56 - loss 0.07 - time 0.27 // val_score  92.18 - val_loss 0.33 - time 0.04\n",
      "Epoch  416 - train_score  98.47 - loss 0.05 - time 0.30 // val_score  92.42 - val_loss 0.36 - time 0.06\n",
      "Epoch  417 - train_score  97.76 - loss 0.06 - time 0.28 // val_score  92.89 - val_loss 0.31 - time 0.04\n",
      "Epoch  418 - train_score  97.15 - loss 0.08 - time 0.26 // val_score  92.89 - val_loss 0.32 - time 0.04\n",
      "Epoch  419 - train_score  97.46 - loss 0.07 - time 0.30 // val_score  92.42 - val_loss 0.36 - time 0.07\n",
      "Epoch  420 - train_score  98.07 - loss 0.05 - time 0.29 // val_score  93.84 - val_loss 0.33 - time 0.04\n",
      "Epoch  421 - train_score  97.46 - loss 0.07 - time 0.27 // val_score  92.89 - val_loss 0.32 - time 0.05\n",
      "Epoch  422 - train_score  97.66 - loss 0.07 - time 0.29 // val_score  92.89 - val_loss 0.33 - time 0.07\n",
      "Epoch  423 - train_score  97.46 - loss 0.07 - time 0.30 // val_score  91.71 - val_loss 0.32 - time 0.03\n",
      "Epoch  424 - train_score  97.05 - loss 0.08 - time 0.25 // val_score  93.13 - val_loss 0.32 - time 0.05\n",
      "Epoch  425 - train_score  97.05 - loss 0.08 - time 0.28 // val_score  91.94 - val_loss 0.32 - time 0.04\n",
      "Epoch  426 - train_score  97.56 - loss 0.07 - time 0.31 // val_score  91.71 - val_loss 0.33 - time 0.05\n",
      "Epoch  427 - train_score  97.56 - loss 0.06 - time 0.25 // val_score  92.18 - val_loss 0.32 - time 0.05\n",
      "Epoch  428 - train_score  98.68 - loss 0.06 - time 0.29 // val_score  93.36 - val_loss 0.31 - time 0.04\n",
      "Epoch  429 - train_score  97.76 - loss 0.06 - time 0.31 // val_score  93.36 - val_loss 0.31 - time 0.04\n",
      "Epoch  430 - train_score  97.86 - loss 0.06 - time 0.26 // val_score  93.36 - val_loss 0.32 - time 0.04\n",
      "Epoch  431 - train_score  97.76 - loss 0.07 - time 0.28 // val_score  93.36 - val_loss 0.32 - time 0.04\n",
      "Epoch  432 - train_score  97.56 - loss 0.07 - time 0.31 // val_score  93.13 - val_loss 0.33 - time 0.04\n",
      "Epoch  433 - train_score  96.95 - loss 0.09 - time 0.28 // val_score  90.28 - val_loss 0.37 - time 0.04\n",
      "Epoch  434 - train_score  96.95 - loss 0.08 - time 0.30 // val_score  93.60 - val_loss 0.32 - time 0.04\n",
      "Epoch  435 - train_score  97.46 - loss 0.07 - time 0.31 // val_score  93.36 - val_loss 0.32 - time 0.04\n",
      "Epoch  436 - train_score  97.46 - loss 0.06 - time 0.27 // val_score  92.42 - val_loss 0.33 - time 0.04\n",
      "Epoch  437 - train_score  97.76 - loss 0.07 - time 0.29 // val_score  93.84 - val_loss 0.31 - time 0.04\n",
      "Epoch  438 - train_score  97.46 - loss 0.07 - time 0.33 // val_score  90.52 - val_loss 0.38 - time 0.04\n",
      "Epoch  439 - train_score  96.85 - loss 0.08 - time 0.27 // val_score  94.08 - val_loss 0.30 - time 0.04\n",
      "Epoch  440 - train_score  97.56 - loss 0.07 - time 0.29 // val_score  91.94 - val_loss 0.35 - time 0.04\n",
      "Epoch  441 - train_score  98.07 - loss 0.06 - time 0.34 // val_score  92.65 - val_loss 0.30 - time 0.04\n",
      "Epoch  442 - train_score  97.46 - loss 0.08 - time 0.26 // val_score  93.84 - val_loss 0.31 - time 0.04\n",
      "Epoch  443 - train_score  97.25 - loss 0.07 - time 0.29 // val_score  93.13 - val_loss 0.31 - time 0.04\n",
      "Epoch  444 - train_score  98.17 - loss 0.06 - time 0.30 // val_score  93.36 - val_loss 0.29 - time 0.04\n",
      "Epoch  445 - train_score  98.17 - loss 0.07 - time 0.25 // val_score  93.60 - val_loss 0.32 - time 0.04\n",
      "Epoch  446 - train_score  97.36 - loss 0.07 - time 0.30 // val_score  92.65 - val_loss 0.31 - time 0.04\n",
      "Epoch  447 - train_score  98.37 - loss 0.05 - time 0.31 // val_score  91.94 - val_loss 0.32 - time 0.04\n",
      "Epoch  448 - train_score  96.85 - loss 0.07 - time 0.26 // val_score  93.13 - val_loss 0.32 - time 0.04\n",
      "Epoch  449 - train_score  97.66 - loss 0.07 - time 0.29 // val_score  91.71 - val_loss 0.33 - time 0.04\n",
      "Epoch  450 - train_score  98.17 - loss 0.06 - time 0.32 // val_score  93.36 - val_loss 0.31 - time 0.03\n",
      "Epoch  451 - train_score  98.17 - loss 0.06 - time 0.27 // val_score  93.13 - val_loss 0.35 - time 0.04\n",
      "Epoch  452 - train_score  97.15 - loss 0.08 - time 0.32 // val_score  93.13 - val_loss 0.33 - time 0.04\n",
      "Epoch  453 - train_score  97.46 - loss 0.07 - time 0.32 // val_score  93.13 - val_loss 0.33 - time 0.04\n",
      "Epoch  454 - train_score  97.97 - loss 0.05 - time 0.26 // val_score  94.08 - val_loss 0.31 - time 0.04\n",
      "Epoch  455 - train_score  97.76 - loss 0.06 - time 0.29 // val_score  93.36 - val_loss 0.34 - time 0.05\n",
      "Epoch  456 - train_score  97.56 - loss 0.06 - time 0.32 // val_score  93.13 - val_loss 0.32 - time 0.04\n",
      "Epoch  457 - train_score  98.88 - loss 0.05 - time 0.26 // val_score  92.65 - val_loss 0.33 - time 0.04\n",
      "Epoch  458 - train_score  97.36 - loss 0.07 - time 0.29 // val_score  91.47 - val_loss 0.34 - time 0.05\n",
      "Epoch  459 - train_score  97.25 - loss 0.09 - time 0.32 // val_score  91.47 - val_loss 0.33 - time 0.03\n",
      "Epoch  460 - train_score  97.97 - loss 0.06 - time 0.27 // val_score  93.60 - val_loss 0.32 - time 0.06\n",
      "Epoch  461 - train_score  97.86 - loss 0.06 - time 0.28 // val_score  93.84 - val_loss 0.31 - time 0.04\n",
      "Epoch  462 - train_score  97.86 - loss 0.06 - time 0.33 // val_score  93.13 - val_loss 0.35 - time 0.04\n",
      "Epoch  463 - train_score  97.15 - loss 0.08 - time 0.27 // val_score  91.94 - val_loss 0.35 - time 0.04\n",
      "Epoch  464 - train_score  97.46 - loss 0.07 - time 0.27 // val_score  92.18 - val_loss 0.33 - time 0.04\n",
      "Epoch  465 - train_score  97.36 - loss 0.06 - time 0.32 // val_score  92.89 - val_loss 0.31 - time 0.04\n",
      "Epoch  466 - train_score  97.36 - loss 0.07 - time 0.25 // val_score  91.23 - val_loss 0.34 - time 0.04\n",
      "Epoch  467 - train_score  98.47 - loss 0.05 - time 0.26 // val_score  94.08 - val_loss 0.32 - time 0.04\n",
      "Epoch  468 - train_score  97.46 - loss 0.07 - time 0.31 // val_score  92.89 - val_loss 0.35 - time 0.05\n",
      "Epoch  469 - train_score  97.86 - loss 0.07 - time 0.26 // val_score  93.36 - val_loss 0.33 - time 0.04\n",
      "Epoch  470 - train_score  97.97 - loss 0.07 - time 0.29 // val_score  92.42 - val_loss 0.34 - time 0.05\n",
      "Epoch  471 - train_score  97.97 - loss 0.06 - time 0.30 // val_score  92.89 - val_loss 0.34 - time 0.05\n",
      "Epoch  472 - train_score  98.68 - loss 0.04 - time 0.26 // val_score  92.89 - val_loss 0.35 - time 0.04\n",
      "Epoch  473 - train_score  98.37 - loss 0.06 - time 0.31 // val_score  92.89 - val_loss 0.34 - time 0.09\n",
      "Epoch  474 - train_score  97.66 - loss 0.07 - time 0.34 // val_score  92.65 - val_loss 0.37 - time 0.04\n",
      "Epoch  475 - train_score  97.66 - loss 0.06 - time 0.25 // val_score  93.13 - val_loss 0.33 - time 0.04\n",
      "Epoch  476 - train_score  98.78 - loss 0.04 - time 0.30 // val_score  92.42 - val_loss 0.35 - time 0.04\n",
      "Epoch  477 - train_score  97.46 - loss 0.06 - time 0.31 // val_score  92.89 - val_loss 0.34 - time 0.06\n",
      "Epoch  478 - train_score  98.07 - loss 0.05 - time 0.25 // val_score  93.84 - val_loss 0.33 - time 0.04\n",
      "Epoch  479 - train_score  96.85 - loss 0.08 - time 0.28 // val_score  91.71 - val_loss 0.36 - time 0.05\n",
      "Epoch  480 - train_score  97.97 - loss 0.06 - time 0.32 // val_score  91.47 - val_loss 0.36 - time 0.04\n",
      "Epoch  481 - train_score  97.36 - loss 0.08 - time 0.26 // val_score  92.42 - val_loss 0.33 - time 0.05\n",
      "Epoch  482 - train_score  97.97 - loss 0.06 - time 0.31 // val_score  92.89 - val_loss 0.32 - time 0.05\n",
      "Epoch  483 - train_score  98.27 - loss 0.06 - time 0.30 // val_score  94.31 - val_loss 0.33 - time 0.04\n",
      "Epoch  484 - train_score  97.56 - loss 0.07 - time 0.26 // val_score  93.60 - val_loss 0.33 - time 0.05\n",
      "Epoch  485 - train_score  97.56 - loss 0.06 - time 0.27 // val_score  93.13 - val_loss 0.32 - time 0.04\n",
      "Epoch  486 - train_score  98.27 - loss 0.06 - time 0.30 // val_score  94.31 - val_loss 0.31 - time 0.04\n",
      "Epoch  487 - train_score  97.97 - loss 0.05 - time 0.26 // val_score  92.18 - val_loss 0.35 - time 0.04\n",
      "Epoch  488 - train_score  98.27 - loss 0.05 - time 0.27 // val_score  91.94 - val_loss 0.34 - time 0.05\n",
      "Epoch  489 - train_score  98.47 - loss 0.05 - time 0.31 // val_score  92.42 - val_loss 0.34 - time 0.05\n",
      "Epoch  490 - train_score  97.86 - loss 0.06 - time 0.26 // val_score  91.94 - val_loss 0.34 - time 0.04\n",
      "Epoch  491 - train_score  97.86 - loss 0.06 - time 0.27 // val_score  93.60 - val_loss 0.33 - time 0.04\n",
      "Epoch  492 - train_score  97.86 - loss 0.06 - time 0.30 // val_score  93.13 - val_loss 0.36 - time 0.06\n",
      "Epoch  493 - train_score  98.27 - loss 0.05 - time 0.27 // val_score  91.47 - val_loss 0.36 - time 0.04\n",
      "Epoch  494 - train_score  97.15 - loss 0.08 - time 0.28 // val_score  91.94 - val_loss 0.35 - time 0.05\n",
      "Epoch  495 - train_score  97.46 - loss 0.07 - time 0.30 // val_score  94.31 - val_loss 0.31 - time 0.06\n",
      "Epoch  496 - train_score  98.47 - loss 0.05 - time 0.27 // val_score  93.13 - val_loss 0.32 - time 0.04\n",
      "Epoch  497 - train_score  98.68 - loss 0.05 - time 0.27 // val_score  94.08 - val_loss 0.31 - time 0.05\n",
      "Epoch  498 - train_score  98.68 - loss 0.05 - time 0.30 // val_score  92.42 - val_loss 0.32 - time 0.05\n",
      "Epoch  499 - train_score  97.66 - loss 0.07 - time 0.28 // val_score  92.42 - val_loss 0.34 - time 0.04\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 500):\n",
    "    train(net, train_loader, i)\n",
    "    valida(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(983, 228)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pd.read_csv('../dados/x_train.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(422, 228)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "pd.read_csv('../dados/x_test.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}